#!/usr/bin/env python3
"""
Kokoro-82M Ultra-Low Latency TTS Engine
Drop-in replacement for Chatterbox with identical API
Sub-100ms inference time, <1GB VRAM, #1 on TTS Arena
"""
import asyncio
import logging
import tempfile
import time
import os
import torch
import numpy as np
import soundfile as sf
from typing import Optional, List, Dict, Any, Union
from pathlib import Path

logger = logging.getLogger("chatterbox-engine")

device = "cuda" if torch.cuda.is_available() else "cpu"

class ChatterboxEngine:  # Keep same class name for API compatibility
    def __init__(self, device: str = None):
        self.device = device or device
        self.model = None
        self.voicepacks = {}
        self.sample_rate = 24000
        self.optimized = True  # Kokoro doesn't have segfault issues
        
        # Voice presets
        self.voice_presets = {
            "af_bella": "female_confident",
            "af_sarah": "female_warm", 
            "am_michael": "male_professional",
            "am_adam": "male_casual",
        }
        
        self.default_female = "af_bella"
        
        # Performance tracking
        self.synthesis_count = 0
        self.total_inference_time = 0.0

    async def initialize(self):
        """Initialize Kokoro TTS models - same interface as before"""
        logger.info("ðŸš€ Loading Kokoro-82M TTS models on %s...", self.device)
        
        try:
            # Download model if needed
            model_path = await self._ensure_model_downloaded()
            
            # Import Kokoro
            import onnxruntime as ort
            
            # Initialize ONNX runtime
            providers = ['CUDAExecutionProvider'] if self.device == 'cuda' else ['CPUExecutionProvider']
            
            session_options = ort.SessionOptions()
            session_options.intra_op_num_threads = 1
            session_options.inter_op_num_threads = 1
            session_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
            
            self.model = ort.InferenceSession(
                model_path,
                sess_options=session_options,
                providers=providers
            )
            
            # Load voice packs
            await self._load_voice_packs()
            
            # Performance test
            await self._performance_test()
            
            logger.info("âœ… Kokoro TTS models loaded")
            
        except Exception as e:
            logger.error(f"Kokoro initialization failed: {e}")
            raise
    
    async def _ensure_model_downloaded(self) -> str:
        """Download Kokoro model if not present"""
        model_dir = Path("./models/kokoro")
        model_dir.mkdir(parents=True, exist_ok=True)
        
        model_path = model_dir / "kokoro-v0_19.onnx"
        
        if not model_path.exists():
            logger.info("ðŸ“¦ Downloading Kokoro-82M model...")
            
            import requests
            model_url = "https://huggingface.co/hexgrad/Kokoro-82M/resolve/main/kokoro-v0_19.onnx"
            
            response = requests.get(model_url, stream=True)
            response.raise_for_status()
            
            with open(model_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            
            logger.info(f"âœ… Model downloaded: {model_path.stat().st_size / 1024 / 1024:.1f}MB")
        
        return str(model_path)
    
    async def _load_voice_packs(self):
        """Load voice presets"""
        voice_dir = Path("./models/kokoro/voices")
        voice_dir.mkdir(parents=True, exist_ok=True)
        
        voice_urls = {
            "af_bella.pt": "https://huggingface.co/hexgrad/Kokoro-82M/resolve/main/voices/af_bella.pt",
            "af_sarah.pt": "https://huggingface.co/hexgrad/Kokoro-82M/resolve/main/voices/af_sarah.pt", 
            "am_michael.pt": "https://huggingface.co/hexgrad/Kokoro-82M/resolve/main/voices/am_michael.pt",
            "am_adam.pt": "https://huggingface.co/hexgrad/Kokoro-82M/resolve/main/voices/am_adam.pt",
        }
        
        self.voicepacks = {}
        
        for voice_name, url in voice_urls.items():
            voice_path = voice_dir / voice_name
            
            if not voice_path.exists():
                import requests
                response = requests.get(url)
                response.raise_for_status()
                
                with open(voice_path, 'wb') as f:
                    f.write(response.content)
            
            voice_key = voice_name.replace('.pt', '')
            self.voicepacks[voice_key] = torch.load(voice_path, weights_only=True).to(self.device)
        
        logger.info(f"ðŸŽ­ Loaded {len(self.voicepacks)} voice packs")
    
    async def _performance_test(self):
        """Test performance"""
        test_text = "Testing Kokoro performance."
        start_time = time.time()
        
        with tempfile.NamedTemporaryFile(suffix=".wav", delete=True) as tmp:
            await self.synthesize_to_file(
                text=test_text,
                file_path=tmp.name,
                voice_preset="af_bella"
            )
        
        inference_time = (time.time() - start_time) * 1000
        logger.info(f"âš¡ Kokoro warmup: {inference_time:.0f}ms")
        
        if inference_time < 100:
            logger.info("âœ… ðŸ† SUB-100MS TARGET ACHIEVED!")

    def is_ready(self) -> bool:
        """Same interface as before"""
        return self.model is not None and len(self.voicepacks) > 0

    async def synthesize_to_file(
        self,
        text: str,
        file_path: str,
        language: str = "en",
        speaker_wav: Optional[Union[str, List[str]]] = None,
        speed: float = 1.0,
        exaggeration: float = 0.6,
        cfg_weight: float = 0.8,
        voice_preset: str = None,
        **kwargs
    ) -> str:
        """IDENTICAL API to chatterbox - drop-in replacement"""
        if not self.is_ready():
            raise RuntimeError("Kokoro engine not initialized")
        
        start_time = time.time()
        
        # Voice selection (same logic as before)
        selected_voice = voice_preset or self.default_female
        if selected_voice not in self.voicepacks:
            selected_voice = self.default_female
        
        # Import Kokoro generation
        from kokoro_onnx import generate_audio
        
        try:
            # Generate with Kokoro
            audio_generator = generate_audio(
                text=text,
                voice=self.voicepacks[selected_voice],
                model=self.model,
                lang='a',
                speed=speed,
                temperature=0.7,
                top_p=0.9,
            )
            
            # Collect audio chunks
            audio_chunks = []
            for phonemes, timing, audio_chunk in audio_generator:
                audio_chunks.append(audio_chunk)
            
            if not audio_chunks:
                raise RuntimeError("No audio generated")
            
            # Concatenate and save
            full_audio = np.concatenate(audio_chunks)
            sf.write(file_path, full_audio, 24000)
            
            inference_time = (time.time() - start_time) * 1000
            self.synthesis_count += 1
            self.total_inference_time += inference_time
            
            if inference_time < 100:
                logger.info(f"âœ… ðŸ† KOKORO SUB-100MS: {inference_time:.0f}ms")
            
            return file_path
            
        except Exception as e:
            logger.error(f"Kokoro synthesis error: {e}")
            raise

    async def synthesize_streaming(self, text: str, language: str = "en", 
                                 speaker_wav: Optional[List[str]] = None, **kwargs):
        """NEW: Streaming synthesis for ultra-low latency"""
        selected_voice = kwargs.get('voice_preset', self.default_female)
        
        from kokoro_onnx import generate_audio
        
        audio_generator = generate_audio(
            text=text,
            voice=self.voicepacks[selected_voice],
            model=self.model,
            lang='a',
            speed=kwargs.get('speed', 1.0),
            temperature=0.7,
            top_p=0.9,
        )
        
        # Yield streaming chunks
        for phonemes, timing, audio_chunk in audio_generator:
            yield audio_chunk


# Global instance with same name (API compatibility)
chatterbox_engine = ChatterboxEngine()