#!/usr/bin/env python3
"""
Optimized Chatterbox TTS Engine Wrapper
Implements torch.compile + CUDA graphs for 2-4x speed improvement
"""
import asyncio
import logging
import time
from typing import Optional, List, Union, AsyncIterator, Dict, Any

import torch
import torchaudio as ta

try:
    from chatterbox.tts import ChatterboxTTS
    from chatterbox.mtl_tts import ChatterboxMultilingualTTS
except Exception as e:
    raise RuntimeError(f"Failed to import Chatterbox TTS: {e}")

logger = logging.getLogger("chatterbox-engine-optimized")


# Performance metrics tracker
class PerformanceMetrics:
    def __init__(self):
        self.generation_times = []
        self.compilation_times = []
        self.chunk_latencies = []
        self.first_chunk_times = []
        
    def add_generation_time(self, time_ms: float):
        self.generation_times.append(time_ms)
        
    def add_compilation_time(self, time_ms: float):
        self.compilation_times.append(time_ms)
        
    def add_chunk_latency(self, time_ms: float):
        self.chunk_latencies.append(time_ms)
        
    def add_first_chunk_time(self, time_ms: float):
        self.first_chunk_times.append(time_ms)
        
    def get_stats(self) -> Dict[str, Any]:
        return {
            "avg_generation_time_ms": sum(self.generation_times) / len(self.generation_times) if self.generation_times else 0,
            "avg_first_chunk_time_ms": sum(self.first_chunk_times) / len(self.first_chunk_times) if self.first_chunk_times else 0,
            "total_generations": len(self.generation_times),
            "total_chunks": len(self.chunk_latencies),
            "compilation_time_ms": sum(self.compilation_times)
        }


class OptimizedChatterboxEngine:
    def __init__(self, device: str = None):
        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")
        self.model: Optional[ChatterboxTTS] = None
        self.multilingual_model: Optional[ChatterboxMultilingualTTS] = None
        self.sample_rate = 24000
        self.compiled = False
        self.optimizations_enabled = {
            "torch_compile": False,
            "cuda_graphs": False,
            "mixed_precision": False,
            "streaming": False
        }
        self.metrics = PerformanceMetrics()
        
        # Optimization parameters
        self.compile_mode = "reduce-overhead"  # Options: default, reduce-overhead, max-autotune
        self.use_bf16 = torch.cuda.is_available() and torch.cuda.is_bf16_supported()
        self.chunk_size = 50  # Tokens per streaming chunk (25-200 range)
        
    async def initialize(self, enable_optimizations: bool = True):
        """Initialize models with optional optimizations"""
        logger.info("ðŸš€ Loading Optimized Chatterbox TTS models on %s...", self.device)
        start_time = time.time()
        
        # Load base models
        self.model = await asyncio.to_thread(
            ChatterboxTTS.from_pretrained, device=self.device
        )
        self.multilingual_model = await asyncio.to_thread(
            ChatterboxMultilingualTTS.from_pretrained, device=self.device
        )
        
        if enable_optimizations:
            await self._apply_optimizations()
            
        load_time = (time.time() - start_time) * 1000
        logger.info("âœ… Optimized Chatterbox TTS models loaded in %.1fms", load_time)
        logger.info("ðŸ”§ Optimizations enabled: %s", self.optimizations_enabled)
        
    async def _apply_optimizations(self):
        """Apply performance optimizations"""
        logger.info("âš¡ Applying performance optimizations...")
        start_time = time.time()
        
        # 1. Mixed Precision Optimization
        if self.use_bf16 and self.device == "cuda":
            logger.info("ðŸŽ¯ Applying mixed precision (bfloat16)...")
            try:
                # Apply to English model
                self.model.t3.to(dtype=torch.bfloat16)
                if hasattr(self.model, 'conds'):
                    self.model.conds.t3.to(dtype=torch.bfloat16)
                    
                # Apply to multilingual model  
                self.multilingual_model.t3.to(dtype=torch.bfloat16)
                if hasattr(self.multilingual_model, 'conds'):
                    self.multilingual_model.conds.t3.to(dtype=torch.bfloat16)
                    
                self.optimizations_enabled["mixed_precision"] = True
                logger.info("âœ… Mixed precision enabled")
            except Exception as e:
                logger.warning("âš ï¸ Mixed precision failed: %s", e)
        
        # 2. Torch.compile with CUDA Graphs
        if self.device == "cuda":
            logger.info("ðŸš€ Compiling models with torch.compile + CUDA graphs...")
            try:
                # Compile critical inference components
                if hasattr(self.model, 't3'):
                    # Compile the T3 step which is the main bottleneck
                    if hasattr(self.model.t3, '_step_compilation_target'):
                        self.model.t3._step_compilation_target = torch.compile(
                            self.model.t3._step_compilation_target,
                            mode=self.compile_mode,
                            fullgraph=True,
                            backend="cudagraphs"
                        )
                    else:
                        # Fallback: compile the entire t3 module
                        self.model.t3 = torch.compile(
                            self.model.t3,
                            mode=self.compile_mode,
                            fullgraph=False  # May have graph breaks
                        )
                        
                # Same for multilingual model
                if hasattr(self.multilingual_model, 't3'):
                    if hasattr(self.multilingual_model.t3, '_step_compilation_target'):
                        self.multilingual_model.t3._step_compilation_target = torch.compile(
                            self.multilingual_model.t3._step_compilation_target,
                            mode=self.compile_mode,
                            fullgraph=True,
                            backend="cudagraphs"
                        )
                    else:
                        self.multilingual_model.t3 = torch.compile(
                            self.multilingual_model.t3,
                            mode=self.compile_mode,
                            fullgraph=False
                        )
                        
                self.optimizations_enabled["torch_compile"] = True
                self.optimizations_enabled["cuda_graphs"] = True
                self.compiled = True
                logger.info("âœ… Torch compilation enabled")
            except Exception as e:
                logger.warning("âš ï¸ Torch compilation failed: %s", e)
        
        # 3. Enable streaming support
        self.optimizations_enabled["streaming"] = True
        
        compile_time = (time.time() - start_time) * 1000
        self.metrics.add_compilation_time(compile_time)
        logger.info("âš¡ Optimizations applied in %.1fms", compile_time)
        
    def is_ready(self) -> bool:
        return self.model is not None and self.multilingual_model is not None

    async def synthesize_to_file(
        self,
        text: str,
        file_path: str,
        language: str = "en",
        speaker_wav: Optional[Union[str, List[str]]] = None,
        speed: float = 1.0,
        exaggeration: float = 0.6,
        cfg_weight: float = 0.8,
        enable_streaming: bool = False
    ) -> str:
        """
        Synthesize text to WAV with optimizations
        """
        start_time = time.time()
        
        if enable_streaming and self.optimizations_enabled["streaming"]:
            # Use streaming synthesis for lower latency
            wav_chunks = []
            first_chunk = True
            
            async for chunk, metrics in self._stream_generate(text, language, speaker_wav, exaggeration, cfg_weight):
                wav_chunks.append(chunk)
                
                if first_chunk:
                    first_chunk_time = (time.time() - start_time) * 1000
                    self.metrics.add_first_chunk_time(first_chunk_time)
                    logger.debug("ðŸŽ¯ First chunk generated in %.1fms", first_chunk_time)
                    first_chunk = False
                    
            # Concatenate all chunks
            wav = torch.cat(wav_chunks, dim=-1) if wav_chunks else torch.zeros(1, 1)
        else:
            # Use standard generation
            wav = await self._generate(text, language, speaker_wav, exaggeration, cfg_weight)
            
        # Ensure proper tensor shape and save
        if wav.ndim == 1:
            wav = wav.unsqueeze(0)
        await asyncio.to_thread(ta.save, file_path, wav, self.sample_rate)
        
        total_time = (time.time() - start_time) * 1000
        self.metrics.add_generation_time(total_time)
        
        return file_path

    async def _stream_generate(
        self,
        text: str,
        language: str,
        speaker_wav: Optional[Union[str, List[str]]],
        exaggeration: float,
        cfg_weight: float,
    ) -> AsyncIterator[tuple[torch.Tensor, Dict[str, Any]]]:
        """
        Streaming synthesis with optimized chunk processing
        """
        kwargs = {"exaggeration": exaggeration, "cfg_weight": cfg_weight}
        if speaker_wav:
            ref = speaker_wav[0] if isinstance(speaker_wav, list) else speaker_wav
            kwargs["audio_prompt_path"] = ref
            
        # Select model based on language
        model = self.multilingual_model if language and language.lower() != "en" else self.model
        if language and language.lower() != "en":
            kwargs["language_id"] = language
            
        # Check if streaming is available
        if hasattr(model, 'generate_stream'):
            # Use native streaming if available
            chunk_start_time = time.time()
            async for chunk, chunk_metrics in asyncio.to_thread(
                model.generate_stream, text, chunk_size=self.chunk_size, **kwargs
            ):
                chunk_time = (time.time() - chunk_start_time) * 1000
                self.metrics.add_chunk_latency(chunk_time)
                
                yield chunk, {"chunk_latency_ms": chunk_time, **chunk_metrics}
                chunk_start_time = time.time()
        else:
            # Fallback: generate full audio and split into chunks
            logger.debug("âš ï¸ Using fallback chunking (no native streaming)")
            wav = await self._generate(text, language, speaker_wav, exaggeration, cfg_weight)
            
            # Split into temporal chunks for streaming effect
            chunk_samples = int(self.sample_rate * 0.2)  # 200ms chunks
            for i in range(0, wav.shape[-1], chunk_samples):
                chunk = wav[..., i:i+chunk_samples]
                if chunk.shape[-1] > 0:
                    yield chunk, {"chunk_index": i // chunk_samples}

    async def _generate(
        self,
        text: str,
        language: str,
        speaker_wav: Optional[Union[str, List[str]]],
        exaggeration: float,
        cfg_weight: float,
    ) -> torch.Tensor:
        """
        Standard generation with optimizations
        """
        kwargs = {"exaggeration": exaggeration, "cfg_weight": cfg_weight}
        if speaker_wav:
            ref = speaker_wav[0] if isinstance(speaker_wav, list) else speaker_wav
            kwargs["audio_prompt_path"] = ref

        if language and language.lower() != "en":
            kwargs["language_id"] = language
            wav = await asyncio.to_thread(self.multilingual_model.generate, text, **kwargs)
        else:
            wav = await asyncio.to_thread(self.model.generate, text, **kwargs)
        return wav
        
    def get_optimization_status(self) -> Dict[str, Any]:
        """Get current optimization status and metrics"""
        return {
            "optimizations_enabled": self.optimizations_enabled,
            "compiled": self.compiled,
            "device": self.device,
            "compile_mode": self.compile_mode,
            "mixed_precision_bf16": self.use_bf16,
            "chunk_size": self.chunk_size,
            "performance_metrics": self.metrics.get_stats()
        }
        
    def configure_streaming(self, chunk_size: int = 50):
        """Configure streaming parameters"""
        if not (25 <= chunk_size <= 200):
            logger.warning("Chunk size %d outside recommended range (25-200), using default", chunk_size)
            chunk_size = 50
        self.chunk_size = chunk_size
        logger.info("ðŸŽ›ï¸ Streaming configured: chunk_size=%d", chunk_size)
        
    async def warmup(self, text: str = "Warmup test for optimized inference."):
        """Warmup compiled models for optimal performance"""
        if not self.compiled:
            logger.info("âš ï¸ Skipping warmup - models not compiled")
            return
            
        logger.info("ðŸ”¥ Warming up compiled models...")
        start_time = time.time()
        
        try:
            # Warmup both models
            await self._generate(text, "en", None, 0.5, 0.8)
            if self.multilingual_model:
                await self._generate(text, "es", None, 0.5, 0.8)  # Warmup multilingual
                
            warmup_time = (time.time() - start_time) * 1000
            logger.info("âœ… Model warmup completed in %.1fms", warmup_time)
        except Exception as e:
            logger.warning("âš ï¸ Warmup failed: %s", e)


# Singleton optimized engine
optimized_chatterbox_engine = OptimizedChatterboxEngine()
