#!/usr/bin/env python3
"""
June TTS Service - OpenAudio S1 (Fish Speech) INTEGRATION
State-of-the-art TTS with emotion control and natural prosody

KEY FEATURES:
- OpenAudio S1 (#1 on TTS-Arena leaderboard)
- 50+ emotion markers: (excited), (happy), (sad), (laughing), etc.
- 150ms streaming latency
- Superior quality vs ElevenLabs
- 14 languages support
- GPU optimized with --compile support
"""
import asyncio
import logging
import os
import sys
import time
import tempfile
import hashlib
import io
import re
from typing import Optional, AsyncGenerator
from pathlib import Path

import numpy as np
import torch
from fastapi import FastAPI, HTTPException, UploadFile, File, Form
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field
from livekit import rtc
import soundfile as sf
import asyncpg

# Fish Speech imports
sys.path.insert(0, '/opt/fish-speech')
from tools.llama.generate import launch_thread_safe_queue, GenerateRequest, GenerateResponse
from tools.vqgan.inference import load_model as load_decoder_model

# -----------------------------------------------------------------------------
# Logging
# -----------------------------------------------------------------------------
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger("june-tts-fish-speech")

# -----------------------------------------------------------------------------
# Configuration - FISH SPEECH OPTIMIZED
# -----------------------------------------------------------------------------
LIVEKIT_IDENTITY = os.getenv("LIVEKIT_IDENTITY", "june-tts")
LIVEKIT_ROOM_NAME = os.getenv("LIVEKIT_ROOM", "ozzu-main")
ORCHESTRATOR_URL = os.getenv("ORCHESTRATOR_URL", "https://api.ozzu.world")

# PostgreSQL
DB_HOST = os.getenv("DB_HOST", "100.64.0.1")
DB_PORT = int(os.getenv("DB_PORT", "30432"))
DB_NAME = os.getenv("DB_NAME", "june")
DB_USER = os.getenv("DB_USER", "keycloak")
DB_PASSWORD = os.getenv("DB_PASSWORD", "Pokemon123!")

# Model paths
CHECKPOINT_PATH = Path("/app/checkpoints/fish-speech-1.5")
LLAMA_CHECKPOINT = CHECKPOINT_PATH / "model.pth"
DECODER_CHECKPOINT = CHECKPOINT_PATH / "firefly-gan-vq-fsq-8x1024-21hz-generator.pth"
DECODER_CONFIG = "firefly_gan_vq"

# Audio settings
FISH_SPEECH_SAMPLE_RATE = 44100  # Fish Speech native sample rate
LIVEKIT_SAMPLE_RATE = 48000
LIVEKIT_FRAME_SIZE = 960  # 20ms @ 48kHz
FRAME_PERIOD_S = 0.020  # 20ms pacing

# Fish Speech inference parameters
COMPILE_MODEL = True  # Enable torch.compile for 10x speedup
MAX_NEW_TOKENS = 1024
CHUNK_LENGTH = 200
TOP_P = 0.7
REPETITION_PENALTY = 1.2
TEMPERATURE = 0.7

# -----------------------------------------------------------------------------
# FastAPI app
# -----------------------------------------------------------------------------
app = FastAPI(
    title="June TTS (OpenAudio S1 / Fish Speech)",
    version="5.0.0",
    description="State-of-the-art TTS with emotion control - #1 on TTS-Arena"
)

# -----------------------------------------------------------------------------
# Global state
# -----------------------------------------------------------------------------
fish_speech_model = None
decoder_model = None
llama_queue = None

livekit_room = None
livekit_audio_source = None
livekit_connected = False

db_pool = None

current_voice_id: Optional[str] = None
current_reference_audio: Optional[bytes] = None

_gpu_resampler = None

# -----------------------------------------------------------------------------
# Schemas
# -----------------------------------------------------------------------------
class SynthesizeRequest(BaseModel):
    text: str = Field(..., min_length=1)
    room_name: str = Field(..., description="LiveKit room name")
    language: str = Field(default="en")
    voice_id: str = Field(default="default")
    # Fish Speech parameters
    temperature: float = Field(default=0.7, ge=0.1, le=1.0)
    top_p: float = Field(default=0.7, ge=0.1, le=1.0)
    repetition_penalty: float = Field(default=1.2, ge=1.0, le=2.0)

# -----------------------------------------------------------------------------
# Emotion marker processing
# -----------------------------------------------------------------------------
SUPPORTED_EMOTIONS = [
    "angry", "sad", "excited", "surprised", "satisfied", "delighted", "scared",
    "worried", "upset", "nervous", "frustrated", "depressed", "empathetic",
    "embarrassed", "disgusted", "moved", "proud", "relaxed", "grateful",
    "confident", "interested", "curious", "confused", "joyful", "happy",
    "laughing", "chuckling", "sobbing", "crying", "sighing", "panting"
]

def validate_emotion_markers(text: str) -> str:
    """
    Validate and preserve emotion markers in text
    Fish Speech supports markers like: (excited)Hello! (laughing)Ha ha!
    """
    # Find all emotion markers
    markers = re.findall(r'\((\w+)\)', text)

    # Log detected emotions
    if markers:
        valid_emotions = [m for m in markers if m.lower() in SUPPORTED_EMOTIONS]
        if valid_emotions:
            logger.info(f"üé≠ Detected emotions: {', '.join(valid_emotions)}")

    # Fish Speech handles emotion markers natively, just return as-is
    return text

# -----------------------------------------------------------------------------
# Helpers
# -----------------------------------------------------------------------------
def compute_hash(data: bytes) -> str:
    """Compute MD5 hash of data"""
    return hashlib.md5(data).hexdigest()[:8]

# -----------------------------------------------------------------------------
# Database
# -----------------------------------------------------------------------------
async def init_db_pool():
    global db_pool
    db_pool = await asyncpg.create_pool(
        host=DB_HOST, port=DB_PORT, database=DB_NAME,
        user=DB_USER, password=DB_PASSWORD, min_size=2, max_size=10
    )
    logger.info("‚úÖ PostgreSQL connection pool created")

    async with db_pool.acquire() as conn:
        await conn.execute("""
            CREATE TABLE IF NOT EXISTS tts_voices (
                id SERIAL PRIMARY KEY,
                voice_id VARCHAR(100) UNIQUE NOT NULL,
                name VARCHAR(255),
                audio_data BYTEA NOT NULL,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)
    logger.info("‚úÖ Voices table ready")

async def get_voice_from_db(voice_id: str) -> Optional[bytes]:
    async with db_pool.acquire() as conn:
        row = await conn.fetchrow(
            "SELECT audio_data FROM tts_voices WHERE voice_id = $1", voice_id
        )
    if row:
        logger.info(f"‚úÖ Loaded voice '{voice_id}' from DB")
        return bytes(row["audio_data"])
    logger.warning(f"‚ö†Ô∏è Voice '{voice_id}' not found in DB")
    return None

# -----------------------------------------------------------------------------
# Voice reference loading
# -----------------------------------------------------------------------------
async def load_voice_reference(voice_id: str = "default"):
    """Load reference audio for voice cloning"""
    global current_voice_id, current_reference_audio

    if voice_id == current_voice_id and current_reference_audio is not None:
        logger.debug(f"‚úÖ Voice '{voice_id}' already cached")
        return True

    logger.info(f"üìÅ Loading voice reference '{voice_id}'...")
    audio_bytes = await get_voice_from_db(voice_id)

    if audio_bytes:
        current_reference_audio = audio_bytes
        current_voice_id = voice_id
        logger.info(f"‚úÖ Voice '{voice_id}' loaded ({len(audio_bytes)} bytes)")
        return True
    else:
        # Use default reference if available
        default_ref = Path("/app/references/June.wav")
        if default_ref.exists():
            with open(default_ref, 'rb') as f:
                current_reference_audio = f.read()
            current_voice_id = voice_id
            logger.info(f"‚úÖ Using default reference audio")
            return True
        else:
            logger.warning(f"‚ö†Ô∏è No reference audio available for '{voice_id}'")
            current_reference_audio = None
            return False

# -----------------------------------------------------------------------------
# Fish Speech Model Loading
# -----------------------------------------------------------------------------
async def load_fish_speech_model():
    """Load Fish Speech models (LLaMA + Decoder)"""
    global fish_speech_model, decoder_model, llama_queue, _gpu_resampler

    logger.info("üêü Loading Fish Speech (OpenAudio S1)...")

    # Check for model files
    if not LLAMA_CHECKPOINT.exists():
        raise FileNotFoundError(f"LLaMA checkpoint not found: {LLAMA_CHECKPOINT}")
    if not DECODER_CHECKPOINT.exists():
        raise FileNotFoundError(f"Decoder checkpoint not found: {DECODER_CHECKPOINT}")

    # Load decoder (vocoder)
    logger.info(f"üì¶ Loading decoder from {DECODER_CHECKPOINT}")
    decoder_model = load_decoder_model(
        config_name=DECODER_CONFIG,
        checkpoint_path=str(DECODER_CHECKPOINT),
        device="cuda" if torch.cuda.is_available() else "cpu"
    )
    logger.info("‚úÖ Decoder loaded")

    # Initialize LLaMA queue for thread-safe inference
    logger.info(f"üì¶ Loading LLaMA model from {LLAMA_CHECKPOINT}")
    llama_queue = launch_thread_safe_queue(
        checkpoint_path=str(LLAMA_CHECKPOINT),
        device="cuda" if torch.cuda.is_available() else "cpu",
        precision=torch.bfloat16 if torch.cuda.is_available() else torch.float32,
        compile=COMPILE_MODEL  # Enable torch.compile for speedup
    )
    logger.info(f"‚úÖ LLaMA model loaded (compile={'ON' if COMPILE_MODEL else 'OFF'})")

    # Setup GPU resampler
    if torch.cuda.is_available():
        import torchaudio.transforms as T
        _gpu_resampler = T.Resample(FISH_SPEECH_SAMPLE_RATE, LIVEKIT_SAMPLE_RATE).cuda()
        logger.info("‚úÖ GPU resampler ready")
    else:
        import torchaudio.transforms as T
        _gpu_resampler = T.Resample(FISH_SPEECH_SAMPLE_RATE, LIVEKIT_SAMPLE_RATE)
        logger.info("‚ö†Ô∏è Using CPU resampler")

    # Load default voice
    await load_voice_reference("default")

    logger.info("‚úÖ Fish Speech models ready")
    return True

# -----------------------------------------------------------------------------
# Fish Speech Inference
# -----------------------------------------------------------------------------
async def synthesize_with_fish_speech(
    text: str,
    reference_audio: Optional[bytes] = None,
    temperature: float = 0.7,
    top_p: float = 0.7,
    repetition_penalty: float = 1.2
) -> AsyncGenerator[np.ndarray, None]:
    """
    Synthesize speech using Fish Speech with streaming

    Args:
        text: Text to synthesize (can include emotion markers)
        reference_audio: Reference audio bytes for voice cloning
        temperature: Sampling temperature
        top_p: Nucleus sampling parameter
        repetition_penalty: Repetition penalty

    Yields:
        Audio chunks as numpy arrays
    """
    global llama_queue, decoder_model

    if llama_queue is None or decoder_model is None:
        raise RuntimeError("Models not loaded")

    # Validate emotion markers
    text = validate_emotion_markers(text)

    # Save reference audio to temp file if provided
    reference_path = None
    if reference_audio:
        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp:
            reference_path = tmp.name
            tmp.write(reference_audio)

    try:
        # Create generation request
        request = GenerateRequest(
            text=text,
            reference_audio=reference_path,
            reference_text=None,  # Let model infer from audio
            max_new_tokens=MAX_NEW_TOKENS,
            chunk_length=CHUNK_LENGTH,
            top_p=top_p,
            repetition_penalty=repetition_penalty,
            temperature=temperature,
            streaming=True  # Enable streaming
        )

        # Get response from LLaMA queue
        logger.info(f"üé§ Generating speech: '{text[:50]}...'")
        response: GenerateResponse = await asyncio.get_event_loop().run_in_executor(
            None, lambda: llama_queue.put(request)
        )

        # Stream audio chunks
        if response.codes is not None:
            # Decode VQ codes to audio using decoder
            codes_tensor = torch.from_numpy(response.codes)
            if torch.cuda.is_available():
                codes_tensor = codes_tensor.cuda()

            with torch.no_grad():
                audio_output = decoder_model.decode(codes_tensor)

                if torch.is_tensor(audio_output):
                    audio_np = audio_output.detach().cpu().numpy()
                else:
                    audio_np = audio_output

                # Ensure correct shape
                if audio_np.ndim > 1:
                    audio_np = audio_np.squeeze()

                # Yield audio chunk
                yield audio_np.astype(np.float32)

    finally:
        # Cleanup temp file
        if reference_path and os.path.exists(reference_path):
            try:
                os.unlink(reference_path)
            except:
                pass

# -----------------------------------------------------------------------------
# LiveKit Connection
# -----------------------------------------------------------------------------
async def get_livekit_token(identity: str, room_name: str):
    import httpx
    url = f"{ORCHESTRATOR_URL}/token"
    payload = {"roomName": room_name, "participantName": identity}
    async with httpx.AsyncClient(timeout=10.0) as client:
        r = await client.post(url, json=payload)
        r.raise_for_status()
        data = r.json()
        ws_url = data.get("livekitUrl") or data.get("ws_url")
        token = data["token"]
        if not ws_url:
            raise RuntimeError(f"Missing livekitUrl")
        return ws_url, token

async def connect_livekit():
    global livekit_room, livekit_audio_source, livekit_connected
    try:
        ws_url, token = await get_livekit_token(LIVEKIT_IDENTITY, LIVEKIT_ROOM_NAME)
        room = rtc.Room()
        await room.connect(ws_url, token)

        source = rtc.AudioSource(LIVEKIT_SAMPLE_RATE, 1)
        track = rtc.LocalAudioTrack.create_audio_track("fish-speech-audio", source)
        opts = rtc.TrackPublishOptions()
        opts.source = rtc.TrackSource.SOURCE_MICROPHONE
        await room.local_participant.publish_track(track, opts)

        livekit_room = room
        livekit_audio_source = source
        livekit_connected = True
        logger.info(f"‚úÖ LiveKit connected at {LIVEKIT_SAMPLE_RATE} Hz")
        return True
    except Exception as e:
        logger.error(f"‚ùå LiveKit connection failed: {e}")
        return False

def resample_audio_fast(chunk: np.ndarray, input_sr: int, output_sr: int) -> np.ndarray:
    global _gpu_resampler
    if input_sr == output_sr:
        return chunk
    try:
        t = torch.from_numpy(chunk).float() if not torch.is_tensor(chunk) else chunk.float()
        if t.dim() == 1:
            t = t.unsqueeze(0)
        if torch.cuda.is_available() and _gpu_resampler is not None:
            t = t.cuda()
        if _gpu_resampler is None:
            import torchaudio.transforms as T
            _gpu_resampler = T.Resample(input_sr, output_sr)
            if torch.cuda.is_available():
                _gpu_resampler = _gpu_resampler.cuda()
        out = _gpu_resampler(t)
        return out.squeeze().detach().cpu().numpy()
    except Exception as e:
        logger.error(f"‚ùå GPU resampling failed: {e}")
        from scipy.signal import resample_poly
        return resample_poly(chunk, output_sr, input_sr)

# -----------------------------------------------------------------------------
# LiveKit Streaming
# -----------------------------------------------------------------------------
async def stream_audio_to_livekit(audio_chunk_generator, sample_rate: int = FISH_SPEECH_SAMPLE_RATE):
    """Stream audio chunks to LiveKit with proper pacing"""
    global livekit_audio_source, livekit_connected

    if not livekit_connected or livekit_audio_source is None:
        logger.warning("LiveKit not connected")
        return

    buffer = np.zeros(0, dtype=np.float32)
    t0 = time.time()
    first = True
    frames_sent = 0

    FRAME_SAMPLES = LIVEKIT_FRAME_SIZE
    FRAME_PERIOD = FRAME_PERIOD_S

    async def send_frame(frame_f32: np.ndarray):
        pcm_int16 = (np.clip(frame_f32, -1.0, 1.0) * 32767).astype(np.int16)
        frame = rtc.AudioFrame.create(LIVEKIT_SAMPLE_RATE, 1, FRAME_SAMPLES)
        np.copyto(np.frombuffer(frame.data, dtype=np.int16), pcm_int16)
        await livekit_audio_source.capture_frame(frame)

    async def producer():
        nonlocal buffer, first
        async for chunk in audio_chunk_generator:
            if torch.is_tensor(chunk):
                chunk = chunk.detach().cpu().numpy()
            if chunk.ndim > 1:
                chunk = chunk.squeeze()

            if sample_rate != LIVEKIT_SAMPLE_RATE:
                chunk = resample_audio_fast(chunk, sample_rate, LIVEKIT_SAMPLE_RATE)

            if first:
                logger.info(f"‚ö° First audio in {(time.time()-t0)*1000:.0f}ms")
                first = False

            buffer = np.concatenate([buffer, chunk.astype(np.float32, copy=False)])

    async def consumer(prod_task: asyncio.Task):
        nonlocal buffer, frames_sent
        next_deadline = time.perf_counter()
        while True:
            if prod_task.done() and buffer.size < FRAME_SAMPLES:
                break

            if buffer.size >= FRAME_SAMPLES:
                frame = buffer[:FRAME_SAMPLES]
                buffer = buffer[FRAME_SAMPLES:]
            else:
                frame = np.zeros(FRAME_SAMPLES, dtype=np.float32)

            await send_frame(frame)
            frames_sent += 1

            # Steady pacing
            next_deadline += FRAME_PERIOD
            now = time.perf_counter()
            delay = next_deadline - now
            if delay > 0:
                await asyncio.sleep(delay)
            else:
                next_deadline = now

    prod_task = asyncio.create_task(producer())
    await consumer(prod_task)
    await prod_task

    logger.info(f"‚úÖ Streamed {frames_sent} frames (~{frames_sent * 20} ms)")

# -----------------------------------------------------------------------------
# Lifecycle
# -----------------------------------------------------------------------------
@app.on_event("startup")
async def on_startup():
    logger.info("=" * 80)
    logger.info("üöÄ Fish Speech TTS Service (OpenAudio S1)")
    logger.info(f"#1 on TTS-Arena | 50+ Emotion Markers | 150ms Latency")
    logger.info(f"")
    logger.info(f"Audio: Fish Speech {FISH_SPEECH_SAMPLE_RATE} Hz ‚Üí LiveKit {LIVEKIT_SAMPLE_RATE} Hz")
    logger.info(f"Compile: {'ENABLED (10x speedup)' if COMPILE_MODEL else 'DISABLED'}")
    logger.info(f"GPU: {'AVAILABLE' if torch.cuda.is_available() else 'CPU ONLY'}")
    logger.info("=" * 80)

    await init_db_pool()
    ok = await load_fish_speech_model()
    if not ok:
        logger.error("Model failed to load")
        return
    asyncio.create_task(connect_livekit())
    logger.info("‚úÖ Service ready (Fish Speech Mode)")

@app.on_event("shutdown")
async def on_shutdown():
    if db_pool:
        await db_pool.close()
        logger.info("‚úÖ Database pool closed")

# -----------------------------------------------------------------------------
# Health
# -----------------------------------------------------------------------------
@app.get("/health")
async def health():
    return {
        "status": "ok",
        "mode": "fish_speech",
        "model": "OpenAudio S1 / Fish Speech 1.5",
        "model_loaded": llama_queue is not None and decoder_model is not None,
        "livekit_connected": livekit_connected,
        "gpu_available": torch.cuda.is_available(),
        "db_connected": db_pool is not None,
        "current_voice": current_voice_id,
        "config": {
            "fish_speech_sample_rate": FISH_SPEECH_SAMPLE_RATE,
            "livekit_sample_rate": LIVEKIT_SAMPLE_RATE,
            "livekit_frame_size": LIVEKIT_FRAME_SIZE,
            "frame_period_ms": FRAME_PERIOD_S * 1000,
            "compile_enabled": COMPILE_MODEL,
            "emotion_support": True,
            "supported_emotions": len(SUPPORTED_EMOTIONS)
        }
    }

# -----------------------------------------------------------------------------
# Voice management
# -----------------------------------------------------------------------------
@app.post("/api/voices/clone")
async def clone_voice(
    voice_id: str = Form(...),
    voice_name: str = Form(...),
    file: UploadFile = File(...)
):
    global db_pool

    try:
        if not file.filename.lower().endswith((".wav", ".mp3", ".flac", ".m4a")):
            raise HTTPException(400, "Only WAV, MP3, FLAC, M4A audio files supported")

        audio_bytes = await file.read()

        # Validate audio duration (10-30 seconds recommended for Fish Speech)
        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp:
            tmp_path = tmp.name
            tmp.write(audio_bytes)
        try:
            audio, sr = sf.read(tmp_path)
            duration = len(audio) / sr
            if duration < 3:
                raise HTTPException(400, f"Audio too short ({duration:.1f}s). Minimum 3s, recommended 10-30s.")
            if duration > 60:
                raise HTTPException(400, f"Audio too long ({duration:.1f}s). Maximum 60s, recommended 10-30s.")
        finally:
            try:
                os.unlink(tmp_path)
            except FileNotFoundError:
                pass

        async with db_pool.acquire() as conn:
            exists = await conn.fetchval(
                "SELECT COUNT(*) FROM tts_voices WHERE voice_id = $1", voice_id
            )
            if exists > 0:
                raise HTTPException(409, f"Voice ID '{voice_id}' already exists")
            await conn.execute("""
                INSERT INTO tts_voices (voice_id, name, audio_data, created_at, updated_at)
                VALUES ($1, $2, $3, CURRENT_TIMESTAMP, CURRENT_TIMESTAMP)
            """, voice_id, voice_name, audio_bytes)

        return JSONResponse({
            "status": "success",
            "voice_id": voice_id,
            "voice_name": voice_name,
            "audio_duration_seconds": round(duration, 2),
            "audio_size_bytes": len(audio_bytes),
            "audio_size_kb": round(len(audio_bytes) / 1024, 2),
            "sample_rate": sr,
            "message": f"Voice '{voice_id}' cloned successfully"
        })
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Voice cloning failed: {e}", exc_info=True)
        raise HTTPException(500, str(e))

@app.get("/api/voices")
async def list_voices():
    async with db_pool.acquire() as conn:
        rows = await conn.fetch("""
            SELECT voice_id, name, length(audio_data) AS size_bytes, created_at, updated_at
            FROM tts_voices
            ORDER BY created_at DESC
        """)
    voices = [{
        "voice_id": r["voice_id"],
        "name": r["name"],
        "size_bytes": r["size_bytes"],
        "size_kb": round(r["size_bytes"] / 1024, 2),
        "created_at": str(r["created_at"]),
        "updated_at": str(r["updated_at"]),
        "is_loaded": r["voice_id"] == current_voice_id
    } for r in rows]
    return {"total": len(voices), "voices": voices, "current_voice": current_voice_id}

@app.get("/api/voices/{voice_id}")
async def get_voice_info(voice_id: str):
    async with db_pool.acquire() as conn:
        row = await conn.fetchrow("""
            SELECT voice_id, name, length(audio_data) AS size_bytes, created_at, updated_at
            FROM tts_voices WHERE voice_id = $1
        """, voice_id)
    if not row:
        raise HTTPException(404, f"Voice '{voice_id}' not found")
    return {
        "voice_id": row["voice_id"],
        "name": row["name"],
        "size_bytes": row["size_bytes"],
        "created_at": str(row["created_at"]),
        "updated_at": str(row["updated_at"]),
        "is_loaded": voice_id == current_voice_id
    }

@app.delete("/api/voices/{voice_id}")
async def delete_voice(voice_id: str):
    if voice_id == current_voice_id:
        raise HTTPException(409, f"Cannot delete currently loaded voice")
    async with db_pool.acquire() as conn:
        result = await conn.execute("DELETE FROM tts_voices WHERE voice_id = $1", voice_id)
    if result == "DELETE 0":
        raise HTTPException(404, f"Voice '{voice_id}' not found")
    return {"status": "success", "message": f"Voice '{voice_id}' deleted"}

# -----------------------------------------------------------------------------
# TTS synthesis - FISH SPEECH
# -----------------------------------------------------------------------------
@app.post("/api/tts/synthesize")
async def synthesize_speech(request: SynthesizeRequest):
    global current_voice_id, current_reference_audio

    start_time = time.time()
    if llama_queue is None or decoder_model is None:
        raise HTTPException(503, "Models not loaded")

    if request.voice_id != current_voice_id:
        logger.info(f"üîÑ Switching voice: {current_voice_id} ‚Üí {request.voice_id}")
        await load_voice_reference(request.voice_id)

    logger.info(f"üéôÔ∏è Fish Speech Synthesis: '{request.text[:60]}...'")
    logger.info(f"   Voice: {request.voice_id}, Language: {request.language}")
    logger.info(f"   Temp: {request.temperature}, Top-p: {request.top_p}")

    try:
        # Generate speech with Fish Speech
        audio_gen = synthesize_with_fish_speech(
            text=request.text,
            reference_audio=current_reference_audio,
            temperature=request.temperature,
            top_p=request.top_p,
            repetition_penalty=request.repetition_penalty
        )

        await stream_audio_to_livekit(audio_gen, sample_rate=FISH_SPEECH_SAMPLE_RATE)

        total_ms = (time.time() - start_time) * 1000
        logger.info(f"‚úÖ Fish Speech synthesis completed in {total_ms:.0f} ms")

        return JSONResponse({
            "status": "success",
            "mode": "fish_speech",
            "model": "OpenAudio S1",
            "total_time_ms": round(total_ms, 2),
            "voice_id": request.voice_id,
            "language": request.language,
            "text_length": len(request.text),
            "params": {
                "temperature": request.temperature,
                "top_p": request.top_p,
                "repetition_penalty": request.repetition_penalty
            }
        })
    except Exception as e:
        logger.error(f"‚ùå Synthesis error: {e}", exc_info=True)
        raise HTTPException(500, str(e))

# -----------------------------------------------------------------------------
# Entrypoint
# -----------------------------------------------------------------------------
if __name__ == "__main__":
    import uvicorn
    try:
        torch.set_num_threads(1)
    except Exception:
        pass
    uvicorn.run(app, host="0.0.0.0", port=8000)
